# SceneReVis

**SceneReVis: Iterative 3D Indoor Scene Generation with Vision-Language Reinforcement Learning**

A closed-loop framework for generating physically plausible and aesthetically coherent 3D indoor scenes through multi-turn iterative refinement. The system combines Vision-Language Model (VLM) reasoning, physics-based validation, and structured tool calls to produce high-quality 3D room layouts.

---

## ğŸ—ï¸ Architecture Overview

SceneReVis operates through an iterative **Render â†’ Evaluate â†’ Revise** loop:

1. **Initial Scene Scaffolding**: Generate room boundaries and functional groups from text prompts
2. **Multi-modal Feedback Injection**: Combine physics feedback (collision/out-of-bounds detection via Trimesh) with VLM layout assessment
3. **Tool-based Scene Editing**: Structured `tool_calls` for `add_object`, `move_object`, `rotate_object`, `scale_object`, `replace_object`, `remove_object`, and `terminate`
4. **Asset Retrieval & Alignment**: Map abstract object descriptions to real 3D models (3D-FUTURE / Objaverse)
5. **Automated Rendering**: Blender-based dual-view rendering (top-down + diagonal perspective)

### Training Pipeline

- **SFT (Supervised Fine-Tuning)**: Train on CoT (Chain-of-Thought) conversation data with scene editing trajectories
- **RL (Reinforcement Learning)**: GRPO-based training with multi-turn scene editing interactions, using voxel-based physics rewards

---

## ğŸ“ Project Structure

```
SceneReVis/
â”œâ”€â”€ infer.py                      # Inference: iterative scene generation (single & batch)
â”‚
â”œâ”€â”€ eval/                         # Evaluation tools
â”‚   â”œâ”€â”€ myeval.py                 # Mesh-based collision & OOB evaluation
â”‚   â”œâ”€â”€ voxel_eval.py             # Voxel-based spatial evaluation
â”‚   â””â”€â”€ vlm_scene_eval.py         # VLM (GPT-4o Vision) multi-dimension evaluation
â”‚
â”œâ”€â”€ utils/                        # Core utilities
â”‚   â”œâ”€â”€ sample.py                 # 3D-FUTURE asset retrieval (SigLIP-based)
â”‚   â”œâ”€â”€ objaverse_retriever.py    # Objaverse asset retrieval (CLIP+SBERT)
â”‚   â”œâ”€â”€ objaverse_glb_manager.py  # Objaverse GLB asset download & caching
â”‚   â”œâ”€â”€ optimize_scene.py         # GPT-assisted scene physics optimization
â”‚   â”œâ”€â”€ scene_editor.py           # Scene editing operations (add/remove/move/etc.)
â”‚   â”œâ”€â”€ format_converter.py       # Scene format conversion (flat â†” grouped)
â”‚   â”œâ”€â”€ blender_renderer.py       # Blender rendering engine
â”‚   â”œâ”€â”€ blender_wrapper.py        # Blender subprocess wrapper
â”‚   â”œâ”€â”€ main_bpy.py               # Blender script entry point
â”‚   â”œâ”€â”€ visualization_3d.py       # 3D visualization (bbox, arrows, grid)
â”‚   â”œâ”€â”€ RL_utils.py               # RL training utilities
â”‚   â”œâ”€â”€ path_config.py            # Unified path configuration manager
â”‚   â”œâ”€â”€ image_merger.py           # Multi-view image composition
â”‚   â””â”€â”€ batch_render_all.py       # Batch rendering helper
â”‚
â”œâ”€â”€ script/                       # Training scripts
â”‚   â”œâ”€â”€ RL/                       # Reinforcement learning
â”‚   â”‚   â”œâ”€â”€ scene_reward.py       # Reward function (voxel-based physics)
â”‚   â”‚   â”œâ”€â”€ scene_editing_interaction.py  # Multi-turn RL interaction handler
â”‚   â”‚   â”œâ”€â”€ run_grpo_B200.sh      # GRPO training launch script
â”‚   â”‚   â””â”€â”€ config/               # RL configuration files
â”‚   â””â”€â”€ sft/                      # Supervised fine-tuning
â”‚       â””â”€â”€ sft_B200.sh           # SFT training launch script
â”‚
â”œâ”€â”€ verl/                         # VERL RL framework (modified fork)
â”‚   â””â”€â”€ verl/
â”‚       â”œâ”€â”€ interactions/         # Multi-turn interaction interfaces
â”‚       â”‚   â”œâ”€â”€ base.py           # Base interaction class
â”‚       â”‚   â””â”€â”€ scene_editing_interaction.py  # Scene editing interaction
â”‚       â”œâ”€â”€ trainer/              # Training orchestration
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ split_prompts/                # Test prompts (550 total across 7 room types)
â”‚   â”œâ”€â”€ bedroom.txt               # 150 prompts
â”‚   â”œâ”€â”€ living_room.txt           # 150 prompts
â”‚   â”œâ”€â”€ dining_room.txt           # 50 prompts
â”‚   â”œâ”€â”€ entertainment_room.txt    # 50 prompts
â”‚   â”œâ”€â”€ gym.txt                   # 50 prompts
â”‚   â”œâ”€â”€ office.txt                # 50 prompts
â”‚   â””â”€â”€ study_room.txt            # 50 prompts
â”‚
â”œâ”€â”€ metadata/                     # Asset metadata
â”‚   â”œâ”€â”€ model_info_3dfuture_assets.json
â”‚   â””â”€â”€ invalid_threed_front_rooms.txt
â”‚
â”œâ”€â”€ requirements_infer_batch.txt  # Inference dependencies
â”œâ”€â”€ setup_env.sh                  # Environment variable setup
â””â”€â”€ quick_install_blender.sh      # Blender 4.0.2 installation
```

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Create conda environment
conda create -n scenerevis python=3.11 -y
conda activate scenerevis

# Install core dependencies
pip install ms-swift vllm accelerate deepspeed
pip install openai azure-identity
pip install trimesh scipy shapely pillow numpy
pip install compress_json compress_pickle open_clip_torch sentence-transformers
pip install swanlab msgspec python-fcl

# Or install from requirements file
pip install -r requirements_infer_batch.txt

# Install Blender 4.0.2 for rendering (no sudo required)
bash quick_install_blender.sh

# (Optional) Install VERL framework for RL training
cd verl && pip install -e . && cd ..
```

### 2. Download Required Assets

#### 3D-FUTURE Models (Required)
Download from [3D-FUTURE](https://tianchi.aliyun.com/specials/promotion/alibaba-3d-future) and extract to your datasets directory.

#### Objaverse GLB Assets (Optional)
Objaverse assets are downloaded on-demand during inference via `utils/objaverse_glb_manager.py`. For evaluation, pre-download is recommended as the evaluation scripts only look at the local cache.

#### Metadata Files
The `metadata/` directory contains JSON metadata for 3D-FUTURE assets. You also need the embeddings pickle file (`model_info_3dfuture_assets_embeds.pickle`) for asset retrieval â€” download it separately due to its size.

### 3. Configuration

```bash
# Set environment variables
source setup_env.sh

# Or set manually:
export PTH_3DFUTURE_ASSETS=/path/to/3D-FUTURE-model
export PTH_ASSETS_METADATA=./metadata/model_info_3dfuture_assets.json
export PTH_ASSETS_EMBED=./metadata/model_info_3dfuture_assets_embeds.pickle

# For Azure OpenAI (optional, for VLM feedback & initial room generation)
export AZURE_OPENAI_ENDPOINT=your_endpoint
export AZURE_OPENAI_SCOPE=your_scope
export AZURE_OPENAI_DEPLOYMENT_NAME=your_deployment_name

# Required for multi-GPU inference
export VLLM_WORKER_MULTIPROC_METHOD=spawn
export VLLM_USE_RAY_SPMD_WORKER=0
export VLLM_USE_RAY_COMPILED_DAG=0
export RAY_IGNORE_UNHANDLED_ERRORS=1
```

### 4. Inference

```bash
# Single scene generation
python infer.py \
    --prompt "Design a cozy bedroom with a queen bed and reading corner" \
    --model /path/to/checkpoint \
    --iterations 10 \
    --generate-room \
    --use-model-for-creation \
    --asset-source objaverse

# Batch inference (sequential processing)
python infer.py \
    --batch-mode \
    --model /path/to/checkpoint \
    --prompts-file split_prompts/bedroom.txt \
    --output ./output/bedroom \
    --iterations 15 \
    --max-history-turns 8 \
    --asset-source objaverse \
    --generate-room \
    --use-model-for-creation \
    --skip-existing
```

### 5. Evaluation

```bash
# Collect final scenes from inference output
SCENES_DIR="./output/bedroom/final_scenes_collection"

# Mesh-based collision & OOB evaluation
python eval/myeval.py \
    --format respace \
    --scenes_dir $SCENES_DIR \
    --models_path /path/to/3D-FUTURE-model \
    --output_dir ./output/bedroom/evaluation

# Voxel-based evaluation
python eval/voxel_eval.py \
    --format respace \
    --scenes_dir $SCENES_DIR \
    --models_path /path/to/3D-FUTURE-model \
    --output_file ./output/bedroom/evaluation/voxel_results.json \
    --voxel_size 0.05

# VLM multi-dimension evaluation (requires Azure OpenAI)
python eval/vlm_scene_eval.py \
    --render-dir ./output/bedroom/rendered \
    --prompts-file split_prompts/bedroom.txt
```

### 6. Training

#### SFT (Supervised Fine-Tuning)

Training data: **SceneChain-12K** â€” 11,444 multi-turn scene editing conversation trajectories with rendered images.

```bash
# Run SFT training
bash script/sft/sft_B200.sh
```

#### RL (Reinforcement Learning with GRPO)

```bash
# Install VERL first
cd verl && pip install -e . && cd ..

# Run GRPO training
bash script/RL/run_grpo_B200.sh
```

---

## ğŸ“Š Evaluation Metrics

| Metric | Description | Tool |
|--------|-------------|------|
| Collision Rate | % of objects with physical overlaps | `myeval.py` / `voxel_eval.py` |
| Out-of-Bounds Rate | % of objects outside room boundaries | `myeval.py` / `voxel_eval.py` |
| VLM Rationality | Scene rationality score (0-100) | `vlm_scene_eval.py` |
| VLM Spatial Layout | Layout quality score (0-100) | `vlm_scene_eval.py` |
| VLM Accessibility | Accessibility score (0-100) | `vlm_scene_eval.py` |

---

## ğŸ”§ Key Dependencies

- **[ms-swift](https://github.com/modelscope/ms-swift)**: Model inference framework (VllmEngine for Qwen2.5-VL)
- **[vLLM](https://github.com/vllm-project/vllm)**: High-performance VLM serving
- **[VERL](https://github.com/volcengine/verl)**: RL training framework (modified fork included)
- **[Trimesh](https://trimsh.org/)**: 3D mesh collision detection
- **[Blender](https://www.blender.org/)**: Scene rendering (v4.0.2)
- **[Shapely](https://shapely.readthedocs.io/)**: 2D geometry operations

---

## ğŸ“„ License

This project is released under the MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ“– Citation

If you find SceneReVis useful in your research, please consider citing:

```bibtex
@article{scenerevis2025,
  title={SceneReVis: Iterative 3D Indoor Scene Generation with Vision-Language Reinforcement Learning},
  author={},
  journal={},
  year={2025}
}
```
